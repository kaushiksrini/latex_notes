\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Homework 1: Solutions\\
600.482/682 Deep Learning\\
Fall 2018}
\author{Kaushik Srinivasan}

\begin{document}
\maketitle
With collaboration with Nathan Vallapureddy

\vspace{5mm}
\begin{enumerate}
	\item 
	\begin{enumerate}
	\item 
	\begin{align*}
		P(y|x) & = \mathcal{N}(ax, s^2) \Rightarrow \prod_i p(y_1|x) \longrightarrow \text{max}\\
		& = \log \bigg(\prod_i^N \frac{1}{\sqrt{2\pi s^2}} e^{\frac{-(y_i - ax)^2}{2x^2}} \bigg)\\
		& = \min \frac{1}{\sqrt{2\pi s^2}} \sum_i \frac{(y_i - ax)^2}{\underbrace{2s^2}_{\text{constant}}}\\
		& = \boxed{\min \sum_i^N (y_i - ax)^2} \Longrightarrow x = \frac{\sum_{i=1}^N y_i}{aN}
	\end{align*}
	\item Maximum a posteriori : $\max P(x|y) = \frac{P(y|x) P(x)}{P(y)}$
	\begin{gather*}
		P(Y) = \sum_j P(y|x_j) \cdot p(x_j) \leftarrow \text{this is a normalizing constant, so we can ignore}
	\end{gather*}
	This means we can instead compute
	\begin{align*}
		\text{argmax} \prod p(y_i|x) \cdot p(x) = & \text{argmax} \bigg(\prod_i^N \frac{1}{\sqrt{2\pi s^2}} e^{\frac{-(y_i - ax)^2}{s^2}} \cdot \frac{1}{\sqrt{2\pi r^2}} e^{\frac{-x^2}{r^2}}\bigg)\\
		= & \min \sum_i \frac{-(y_i - ax)^2}{2s^2} + \frac{-x^2}{2r^2}\\
		= & \min\bigg( \frac{x^2}{r^2} + \frac{1}{s^2} \sum_{i=1}^N (y_i - ax)^2 \bigg)
	\end{align*}
	Differentiate w.r.t. $x$ and set to 0 
	\begin{gather*}
		\frac{2x}{r^2} + \frac{1}{s^2} \sum_{i=1}^N -2a(y_i - ax) = 0\\
		\rightarrow \frac{x}{r^2} = \frac{a}{s^2}\sum_{i=1}^N (y_i - ax)\\
		\rightarrow \frac{x}{r^2} + \frac{a^2Nx}{s^2} = \frac{a}{s^2} \sum_{i=1}^N y_i\\
		\boxed{x = \frac{\frac{a}{s^2} \sum_{i=1}^N y_i}{\frac{1}{r^2} + \frac{a^2N}{s^2}}}
	\end{gather*}
	\item When $a = 1, s = 1$, and $r = 1$ and $y \in \{0.45, 0.13, -0.26, 1.27, -0.87, -0.49, -0.12, 0.23 \}$. Then maximum likelihood estimation is.
	\begin{gather*}
		\textbf{MLE:} \qquad \sum_{i=1}^N y_i = 0.34 \quad \text{hence} \quad x = \frac{0.34}{8} = \boxed{0.0425}\\
		\textbf{MAP:} \qquad \sum_{i=1}^N y_i = 0.34 \quad \text{hence} \quad x = \frac{0.34}{1 + 8} \approx \boxed{0.0378}  \\
	\end{gather*}
	\end{enumerate}
	\item 
	\begin{enumerate}
	\item 
	\begin{gather*}
		f(x;\theta)y = y\sum_j \theta_j x^{(j)}\\
		\boxed{\frac{\partial}{\partial \theta_j} f(x;\theta)y = x^{(j)}y}
	\end{gather*}
	\item let M be the set with all misclassified samples, then
	\begin{equation*}
		\boxed{\sum_{i \in M} x_i^{(j)} y}
	\end{equation*}
	\item Code in file
	\item Code in file
	\item The Answers are
	\begin{itemize}
		\item Data 1 - converges in 5 epochs - [[ 0.00312885], [ 0.02170075], [-0.025     ]]
		\item Data 2 - doesn't converge, - min error rate after 29 epochs (0.25) - [[ 0.01895395],[ 0.00178105],[-0.02]]
		\item Data 3 - doesn't converge - min error rate function after 66 epochs (0.15) - [[ 0.00988327], [-0.01209465], [-0.00631251], [ 0.01      ]]
		\item Data 4 - converges in 7 epochs - [[ 0.00564839],[ 0.00030848], [ 0.02565065], [-0.03      ]]
		\item Data 5 - converges in 6 epochs - [[ 0.03937346],[-0.00899502], [ 0.00273425], [-0.03372524], [ 0.01      ]]
		\item Data 6 - doesn't converge - min error rate after 11 epochs (0.15) - [[ 0.00790035], [ 0.01804779], [-0.02766167], [-0.02559074], [ 0.01      ]]
	\end{itemize}
	\end{enumerate}
	\item 
	\begin{enumerate}
	\item 
	\begin{gather*}
		P(y|x;\theta) = \sigma(\theta^T x)\\
		O(D; \theta) = \prod_i P(y_i|x_i) = \prod_i z_i^{y_i} (1-z_i)^{(1-y_i)} \tag{$z_i = \sigma(\theta^T x)$}\\
		\boxed{-\log (O(D)) = -\sum_i \big[y_i \log (z_i) + (1-y_i)\log (1-z_i) \big]} \quad \leftarrow \text{cross entropy}
	\end{gather*}
	\item For simplicity, assume for one example $(x,y)$ where $i$ is fixed, then we can generalize to all the examples. I will also include the negative of the sign in the final step
	\begin{align*}
		\log (O(D)) & =  y \log (\sigma(\theta^T x)) + (1-y)\log (1-\sigma(\theta^T x)) \\
		\frac{\partial}{\partial \theta_j} \log (\theta) & = \bigg(y \frac{1}{\sigma(\theta^T x)} - (1-y) \frac{1}{1-\sigma(\theta^T x)} \bigg) \cdot \frac{\partial}{\partial \theta_j} \sigma(\theta^T x)\\
		& = \bigg(\frac{y (1-\sigma(\theta^T x)) -(1-y)\sigma(\theta^T x)}{\sigma(\theta^T x) (1-\sigma(\theta^T x))} \bigg) \cdot  \sigma(\theta^T x) \cdot (1-\sigma(\theta^T x)) \cdot x^{(j)}\\
		& = \big[ y (1-\sigma(\theta^T x)) -(1-y)\sigma(\theta^T x) \big] x^{(j)}\\
		& = [y - y\sigma(\theta^T x) - \sigma(\theta^T x) + y\sigma(\theta^T x)] x^{(j)}\\
		& = [y - \sigma(\theta^T x)] x^{(j)}
	\end{align*}
	Now if we include all the examples and the negation
	\begin{equation*}
		\boxed{\frac{\partial}{\partial \theta_j} -\log (\theta) = \sum_i [\sigma(\theta^T x_i) - y_i] x_i^{(j)}}
	\end{equation*}
	\item Please see code in attached file
	\item The results from the data are:
	\begin{itemize}
		\item Data 1 - converges in 51 epochs - [[ 0.49006907], [ 3.83194682], [-4.26828716]]
		\item Data 2 - doesn't converge, - min error rate after 28 epochs (0.25) - [[ 1.39005559], [-0.00601565], [-1.33555841]]
		\item Data 3 - doesn't converge - min error rate function after 3 epochs (0.25) - [[ 0.43290483], [-0.44922362], [-0.04366681], [-0.02600216]]
		\item Data 4 - converges in 76 epochs - [[ 1.32904062], [-0.43986865], [ 4.88545346], [-4.97437984]]
		\item Data 5 - converges in 9 epochs - [[ 2.00545512], [-0.23228308], [ 0.13208157], [-1.50019973], [ 0.23684892]]
		\item Data 6 - doesn't converge - min error rate after 1 epochs (0.25) - [[ 0.12201986], [ 0.11603158], [-0.10142059], [-0.43052066], [ 0.02348147]]\\
	\end{itemize}
	\end{enumerate}
	\item 
	\begin{enumerate}
	\item Cost function for $n$ examples, 
	\begin{equation*}
		\boxed{J(\theta) = \frac{1}{n} \sum_{i=1}^n (\theta^Tx_i - y_i)^2}
	\end{equation*}
	\item the Gradient Descent Rule for this cost function is
	\begin{equation*}
		\frac{\partial}{\partial \theta_j} J(\theta) = \frac{2}{n} \sum_{i=1}^n (\theta^Tx_i - y_i) \cdot x_i^{(j)} 
	\end{equation*}
	And so the update rule for a given $\theta_j$ is
	\begin{equation*}
		\theta_j := \theta_j - \alpha \sum_{i=1}^n (\theta^Tx_i - y_i) \cdot x_i^{(j)}
	\end{equation*}
	
	\item Code is in attached file
	\item 
	The values of theta ($\theta$): [[3.18654211], [0.79760108]] where the slope = 3.18654211 and intercept = 0.79760108
	\end{enumerate}
\end{enumerate}


\end{document}

