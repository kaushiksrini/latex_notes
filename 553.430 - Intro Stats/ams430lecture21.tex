\classheader{2018-11-26}
\begin{itemize}
	\item Two sample testing
	\item ANOVA
\end{itemize}
\section*{Two-Sample Testing}
\begin{gather*}
		X_1, X_2, \ldots, X_n \sim \mathcal{N}(\mu_1, \sigma_1^2)\\
	Y_1, Y_2, \ldots, Y_m \sim \mathcal{N}(\mu_2, \sigma_2^2)\\
	\text{Suppose } \sigma_1^2 = \sigma_1^2 = \sigma^2
\end{gather*}
But $\sigma^2$ unknown (common variance assumption). Consider
\begin{align*}
	H_0: & \mu_1 - \mu_2 = 0\\
	H_1: & \mu_1 - \mu_2 \neq 0
\end{align*}
Can compute GLR (HW): hint
\begin{gather*}
	f(x_1, \ldots, x_n, y_1, \ldots, y_m | \mu_1, \mu_2, \sigma^2) = \bigg( \frac{1}{\sigma\sqrt{2\pi}}\bigg)^{n+m} \exp \bigg( 
	\frac{-\sum_{i=1}^n}{2\sigma^2} (x_i - \mu_1)^2 \bigg) \cdot \bigg( -\frac{1}{2\sigma^2} \sum_{j=1}^m (y_j - \mu_2)^2  \bigg)
\end{gather*}
In the homework, you will show that GLRT involves rejecting $H_0$ if 
\begin{gather*}
	\Bigg| 
	\frac{\bX - \bar{Y} - 0}{\sqrt{s_p^2 \big(\frac{1}{n} + \frac{1}{m} \big)}} \Bigg| \quad \text{is sufficiently large}\\\\
	Var(\bX) = \frac{\sigma^2}{n} \quad 	Var(\bar{Y}) = \frac{\sigma^2}{m} \qquad 	Var(\bX - \bar{Y}) = \frac{\sigma^2}{n} + \frac{\sigma^2}{m} = \sigma^2\bigg(\frac{1}{n} + \frac{1}{m}\bigg)\\\\
	s_p^2 = \frac{S_X^2 (n-1) + S_y^2 (m-1)}{n+m-2} = \underbrace{\frac{S_X^2 (n-1)}{(n-1+m-1)} + \frac{S_y^2 (m-1)}{(n-1 + m-1)}}_{\text{weighted average of sample variances}}
\end{gather*}
Under $H_0$, 
\begin{gather*}
	\frac{\bX - \bar{Y} - 0}{\sqrt{s_p^2 \big(\frac{1}{n} + \frac{1}{m} \big)}} \sim t(n+m-2 \text{ df}) \tag{$\star$}
\end{gather*}
Rejecting $H_0$ for large \emph{ABSOLUTE} values of $\star$ is equivalent to rejecting $H_0$ for large values of $(\star)^2$ i.e. rejecting $H_0$ when an $F$-distributed r.v. with 1 num df and $n+m-2$ denominator d.f. is large.

\begin{tikzpicture}
\begin{axis}[
  no markers, domain=0:10, samples=100,
  axis lines*=left, xlabel=$x$, ylabel=$y$,
  every axis y label/.style={at=(current axis.above origin),anchor=south},
  every axis x label/.style={at=(current axis.right of origin),anchor=west},
  height=5cm, width=12cm,
  xtick={4}, ytick=\empty,
  enlargelimits=false, clip=false, axis on top,
  grid = major
  ]
  \addplot [fill=cyan!20, draw=none, domain=5:10] {gauss(4,1)} \closedcycle;
  \addplot [very thick,cyan!50!black] {gauss(4,1)};

\end{axis}

\end{tikzpicture}
\begin{center}
	$\boxed{F_\alpha}$ \qquad Rejecting for large values of $\star^2 \Rightarrow$ upper tailed rejection region
\end{center}
\underline{Summary}: To test equality of means for two independently sampled normal population with equal variances, we use an upper tailed $F$ test, in essence, compares the variability in sample means ($\bX - \bar{Y}$) against the "inherent" variability in the two populations (i.e. using $s_p^2$ as an estimate for $\sigma^2$)\\
\subsubsection*{Now consider comparing multiple population means}
We consider \circled{$Y_{ij}$} = $j^{th}$ data point/measurement from $i^{th}$ sample ($i^{th}$ treatment)\\\\
Suppose we have $I$ populations/treatments, $J$ measurements per treatment = common sample size\\\\
\textbf{Assume}:
\begin{gather*}
	Y_{ij} \sim \underbrace{\mu_i}_{\substack{\text{fixed} \\ \text{deterministic}}} + \underbrace{\epsilon_{ij}}_{\substack{\text{random} \\ \text{error}}}\\
	\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2) \quad \text{\underline{all independent}} \quad \text{common variance: \underline{homoscedasticity}}
\end{gather*}
So $Y_{ij}$ are all independent
\begin{gather*}
	Y_{ij} \sim \mathcal{N}(\mu_i, \sigma^2) \quad \text{for } 1 \leq j \leq J\\
	\text{Consider } H_0 : \underbrace{\mu_1 = \mu_2 = \cdots = \mu_I}_{\text{all means equal}}
\end{gather*}
To make things a bit simpler, let's consider this parametrization:
\begin{gather*}
	Y_{ij} \sim \mu + \alpha_i + \epsilon_{ij}\\
	\text{where: } \sum_{i=1}^I \alpha_i = 0 \quad \& \quad  \epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)
\end{gather*}
Can always do this by setting $\boxed{\mu = \hat{\mu} = \frac{\sum \mu_i}{I}}$ \quad $\alpha_i = (\mu_i - \mu)$
\begin{gather*}
	\sum_{i=1}^I (\mu_i - \mu) = \sum \mu_i - I \mu = \sum \mu_i - I \bar{\mu}= 0
\end{gather*}
We will break up the variability in data into: 
\begin{enumerate}[label=\protect\circled{\arabic*}]
	\item A measure of variability across sample treatment means $\rightarrow$ $SS_B$ ("sum of squares between")
	\item A measure of inherent variability (variability within each treatment) $\rightarrow$ $SS_W$ ("sum of squares within")
\end{enumerate}
\begin{gather*}
	\text{Total sum of squares} = \quad \sum_i \sum_j (Y_{ij} - \bar{Y}_{\cdot \cdot})^2\\
	\text{Key Identity} \quad \sum_i \sum_j (Y_{ij} - \bar{Y}_{\cdot \cdot})^2 = \sum_i \sum_j (Y_{ij} - \bar{Y}_{i\cdot} + \bar{Y}_{i\cdot} - \bar{Y}_{\cdot \cdot})^2\\
\end{gather*}