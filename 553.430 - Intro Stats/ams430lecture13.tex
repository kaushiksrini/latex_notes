\classheader{2018-10-17}
\begin{itemize}
	\item Asymptotic normality of MLEs (8.5)
	\item Efficiency \& Sufficiency (8.7)
	\item Bayesian Estimation (8.6)
\end{itemize}
Suppose $X_i$ are i.i.d. $f(x | \theta)$ where $f$ satisfies regularity conditions 1) smoothness 2) $supp f$ is independent of $\theta$)\\
Let $\hat{\theta}$ be MLE for $\theta$ suppose true value of $\theta$ is $\theta = \theta_0$. Then
\begin{gather*}
	\sqrt{n I(\theta_0)} (\hat{\theta} - \theta_0) \xrightarrow[d]{n \rightarrow \infty} \mathcal{N} (0,1)
\end{gather*}
Note $Var(\hat{\theta})$ is asymptotically given by $\frac{1}{nI(\theta_0)}$
\begin{equation*}
	\sqrt{nI(\theta_0)} (\hat{\theta} - \theta_0) = \frac{\hat{\theta} - \theta_0}{1/nI(\theta_0)}
\end{equation*}
\textbf{Recall:} (where $\ell(\theta)$ is log likelihood)
\begin{equation*}
	\sqrt{n}(\hat{\theta} - \theta_0) \approx \frac{n^{-1/2}\ell ' (\theta_0)}{n^{-1} \ell '' (\theta_0)}
\end{equation*}
\textbf{Recall:} last time we showed
\begin{equation*}
	Var(n^{1/2} \ell ' (\theta_0)) = I(\theta_0)
\end{equation*}
Also the denominator is
\begin{equation*}
	\frac{1}{n} \ell '' (\theta_0) = \frac{1}{n} \sum_{i=1}^n \bigg[ \frac{\partial^2}{(\partial \theta)^2} \log f(X_i | \theta) \bigg] \bigg|_{\theta = \theta_0}
\end{equation*}
By LLN, this converges to
\begin{equation*}
	\mathbb{E} \bigg[ \frac{\partial^2}{(\partial \theta)^2} \log f(X_i | \theta)  \bigg|_{\theta = \theta_0} \bigg] = + I(\theta_0)
\end{equation*}
So we've written
\begin{equation*}
	\sqrt{n} (\hat{\theta} - \theta_0) \approx \frac{W^{(n)}}{U^{(n)}}
\end{equation*}
We know that $U^{(n)} \rightarrow I(\theta_0)$ in probability But what is the numerator?
\begin{equation*}
	W^{(n)} = \frac{1}{\sqrt{n}} \sum_{i=1}^n \underbrace{\bigg[ \frac{\partial}{\partial \theta} \log f(X_i | \theta)\bigg] \bigg|_{\theta = \theta_0}}_{Y_i}
\end{equation*}
Observe that $Y_i$'s are ii, $E[Y_i] = 0; Var(Y_i) = I(\theta_0)$ So by CLT applied to $\frac{1}{\sqrt{n}} \sum\limits_{i=1}^n Y_i$, we find that
\begin{equation*}
	\frac{1}{\sqrt{n I(\theta_0)}} \sum Y_i \xrightarrow{d} \mathcal{N}(0,1)
\end{equation*}
So Slutsky's theorem $\Rightarrow \sqrt{n} (\hat{\theta} - \theta_0) \xrightarrow{d} \mathcal{N} (0, ?)$ \qquad What is \circled{?}\\\\
So we've written 
\begin{gather*}
	[\sqrt{n} (\hat{\theta} - \theta_0) ] \sqrt{I(\theta_0)} \approx \frac{W^{(n)}}{U^{(n)}} \quad (\sqrt{I(\theta_0)})\\
	\text{Notice that} \qquad \frac{\sqrt{I(\theta_0)}}{U^{(n)}} \rightarrow \frac{1}{\sqrt{I(\theta_0)}} \qquad \text{in probability}\\
	\text{Note that} \qquad \frac{W^n}{\sqrt{I(\theta_0)}} = \frac{1}{\sqrt{I(\theta_0)}} \sum Y_i \longrightarrow \mathcal{N}(0, 1)
\end{gather*}
\textbf{So what did we do?}
\begin{enumerate}
	\item First, we did a Taylor expansion (1st order) of log likelihood
	\item We used that to write 
	\begin{gather*}
		\sqrt{n}(\hat{\theta} - \theta_0) \approx -\frac{n^{-1/2}\ell ' (\theta_0)}{n^{-1} \ell '' (\theta_0)}\\
		\textbf{Note: } \qquad \qquad \sqrt{n I(\theta_0)} (\hat{\theta} - \theta_0) \approx \frac{\frac{1}{\sqrt{n I(\theta_0)}} \ell ' (\theta_0)}{\boxed{\frac{-1}{I(\theta_0)} \cdot \frac{1}{n} \ell '' (\theta_0)}}
	\end{gather*}
	\item We used Central Limit Theorem to conclude that
	\begin{equation*}
		\frac{1}{\sqrt{n}} \ell ' (\theta_0) \rightarrow \mathcal{N} (0, I(\theta_0))
	\end{equation*}
	\item By LLN, boxed piece converges in probability to $1/ \sqrt{I(\theta_0)}$
	\item By Slutsky's Theorem, $\sqrt{n I(\theta_0)} (\hat{\theta} - \theta_0) \xrightarrow[d]{n \rightarrow \infty} \mathcal{N} (0,1) $
\end{enumerate}
\subsubsection*{Next: Surprising!}
Suppose that $X_i \sim f(X_i|\theta)$ satisfying regularity conditions and let $T = r(X_1, \ldots, X_n)$ an estimator for $\theta$ Suppose that $T$ is unbiased for $\theta$. ($T$ is not necessarily MLE or MOM...) Then
\begin{equation*}
	Var(T) \geq \frac{1}{nI(\theta)}
\end{equation*}
This is a remarkable \underline{lower bound} on the variance of an unbiased estimator! An unbiased estimator $T$ $(T = T_n = r(X_1, \ldots, X_n))$ Such that $Var(T_n) = \frac{1}{nI(\theta)}$ is said to be efficient
\begin{equation*}
	\text{if} \qquad \frac{Var(T_n)}{1/n I(\theta_0)} \xrightarrow{n \rightarrow \infty} 1, \qquad \text{then $T_n$ is \underline{asymptotically} efficient}
\end{equation*}
\underline{Relative Efficiency:} If we have two unbiased estimators $\hat{\theta_1}$ and $\hat{\theta_2}$, their relative efficiency is the ratio $\frac{Var(\hat{\theta_1})}{Var(\hat{\theta_2})}$\\\\
The \underline{asymptotic relative efficiency} is the limit of this ratio as $n \rightarrow \infty$:
\begin{equation*}
	\lim\limits_{n \rightarrow \infty}  \frac{Var(\hat{\theta_1})}{Var(\hat{\theta_2})}
\end{equation*}
So far we've shown
\begin{enumerate}
	\item MLEs are consistent
	\item MLEs are asymptotically unbiased
	\item MLEs are asymptotically normal
	\item MLEs are asymptotically efficient
\end{enumerate}
\subsection*{Sufficiency}
Let $X_i \sim f(x|\theta)$. Suppose $T = r(X_1, \ldots, X_n)$ is a statistic (i.e. a function of $X_1, \ldots, X_n$) We say T is sufficient for $\theta$ if the conditional distribution of $X_1, \ldots, X_n$ given $T$ is independent of $\theta$
\begin{theorem}
	(Factorization)\\ A statistic $T$ is sufficient for a parameter $\theta$ \textbf{iff} $f(x_1, \ldots, x_n | \theta) = g(T, \theta) \cdot h(X_1, \ldots, X_n)$ 
\end{theorem}