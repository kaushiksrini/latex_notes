\classheader{2018-10-22}
\underline{Sufficiency:} We say that a statistic $T$ is sufficient for the parameter $\theta$ if the conditional distribution of the data $X_1, X_2, \ldots, X_n$ given $T$ does not depend on $\theta$.\\\\
\underline{Factorization Theorem:} A statistic $T$ is sufficient for a parameter $\theta$ \textbf{\textit{iff}} the joint density can be factorized
\begin{equation*}
	f(x_1, \ldots, x_n | \theta) = g(T, \theta) \cdot h(X_1, \ldots, X_n)
\end{equation*}
\begin{remark}
	Sufficient statistic need not be unique and many cases $h(x_1, \ldots, x_n) = 1$
\end{remark}
\begin{example-N}
	Let $X_i$ be i.i.d. Bernoulli($p$). Suppose $n = 3$. Let $T = X_1 + X_2 + X_3$.\\
	\emph{Claim:} $T$ is sufficient for $p$. Let's look at an example
	\begin{gather*}
		P(X_1 = 1, X_2 = 0, X_3 = 1 | T = t)
		\begin{cases}
			0 & \text{ if } t \neq 2\\
			\frac{1}{\binom{3}{2}} & \text{ if } t = 2
		\end{cases} \quad \{t = 2\} = \frac{P(X_1 = 1, X_2 = 0, X_3 = 1 | T = 2)}{P(T = 2)} = \frac{p^2q}{\binom{3}{2} p^2q}
	\end{gather*}
\end{example-N}
Can also invoke \underline{Factorization}: 
\begin{align*}
	p(x_1, \ldots, x_n | \theta) & = \theta^{\sum x_i} \cdot (1-\theta)^{\sum x_i}\\
	& = \underbrace{\Theta^T (1 - \Theta)^{n-T}}_{g(T, \theta)} \cdot \underbrace{1}_{h(x_1, \ldots, x_n)}
\end{align*}
\subsubsection*{Two Paradigms for Statistical Inference}
\begin{enumerate}[label=\protect\circled{\arabic*}]
	\item \textbf{Frequentist:} parameters are unknown \underline{non-random variables}.\\ \emph{Goal}: obtain estimate $T(X_1, \ldots, X_n)$ for this parameter and try to extract useful properties â€” consistency, asymptitic distributions, unbiasedness, minimum variance, \ldots Might want CIs for $\theta$ based on asymptotic distribution of $T$.
	\item \textbf{Bayesian}: parameters are themselves random variables and these parameters have some probability distribution, $f_\lambda(\theta)$, this distribution might involve other parameters, called hyperparameters (often known).\\ This distribution models uncertainty in your belief about $\theta$. It is called a \emph{prior}.\\
\end{enumerate}
Next we have $X_i$ i.i.d. $f(x | \theta)$. This is our data, and $f(x_1, \ldots, x_n | \theta)$ is our joined likelihood (common thread in both paradigms).\\\\
\emph{Goal:} Use the observed data to recalculate conditional probabilities for $\theta$ given observed data i.e. to calculate a \underline{posterior distribution} $f(\theta | x_1, \ldots, x_n)$\\\\
Then we use posterior distribution to extract information about $\theta$ include estimates (for $\theta$):
\begin{enumerate}
	\item posterior mean
	\item posterior median
	\item posterior mode
\end{enumerate}
\begin{example-N}
	Suppose $X_i$ i.i.d. Bernoulli($p$).\\
	\begin{multicols}{2}
		Suppose $p$ satisfies a discrete prior:
	\begin{gather*}
		p = \mathlarger{\begin{cases}
			\frac{1}{4} & \text{w.p. }\frac{1}{3}\\
			\frac{1}{2} & \text{w.p. }\frac{1}{3}\\
			\frac{3}{4} & \text{w.p. }\frac{1}{3}
		\end{cases}}
	\end{gather*}
	An example of continuous, "non-informative" prior:
	\begin{equation*}
		p \sim \text{Unif}(0,1)
	\end{equation*}
	\end{multicols}
	Given $p$, let $X_i \sim$ i.i.d Bernoulli($p$) \qquad $X_1 = 1, X_2 = 1, X_3 = 1, X_4 = 1$\\\\
	Let's calculate posterior distribution of $p$:
	\begin{align*}
		P( p = p_0| X_1 = 1, X_2 = 1, X_3 = 1, X_4 = 1)
		& = \frac{P( p = p_0, X_1 = 1, X_2 = 1, X_3 = 1, X_4 = 1)}{\underbrace{P(X_1 = 1, X_2 = 1, X_3 = 1, X_4 = 1)}_{\text{function of the data} \rightarrow C(X_1, \ldots, X_n)}}\\
		& = \frac{P(X_1 = 1, X_2 = 1, X_3 = 1, X_4 = 1 | p = p_0) \cdot P(p = p_0)}{\sum\limits_{\text{all }a} P(X_1 = 1, X_2 = 1, X_3 = 1, X_4 = 1 | p = a) \cdot P(p = at)} 
	\end{align*}
	Now continue with the example
	\begin{align*}
		Pr(p = \frac{1}{4} | 1,1,1,1)  & = \frac{Pr(1,1,1,1 | p = \frac{1}{4}) \cdot Pr(p = \frac{1}{4})}{Pr(1,1,1,1)}\\
		& = \frac{(1/4)^4 \cdot (1/3)}{(1/4)^4 \cdot (1/3) + (1/2)^4 \cdot (1/3) + (3/4)^4 \cdot (1/3)} = u_1\\\\
		Pr(p = \frac{1}{2} | 1,1,1,1)  & = \frac{Pr(1,1,1,1 | p = \frac{1}{2}) \cdot Pr(p = \frac{1}{2})}{Pr(1,1,1,1)}\\
		& = \frac{(1/2)^4 \cdot (1/3)}{(1/4)^4 \cdot (1/3) + (1/2)^4 \cdot (1/3) + (3/4)^4 \cdot (1/3)} = u_2\\\\
		Pr(p = \frac{3}{4} | 1,1,1,1)  & = \frac{Pr(1,1,1,1 | p = \frac{3}{4}) \cdot Pr(p = \frac{3}{4})}{Pr(1,1,1,1)}\\
		& = \frac{(3/4)^4 \cdot (1/3)}{(1/4)^4 \cdot (1/3) + (1/2)^4 \cdot (1/3) + (3/4)^4 \cdot (1/3)} = u_3
	\end{align*}
	\begin{multicols}{2}
	\begin{gather*}
		p_{\text{post}} \begin{cases}
			1/4 & u_1\\
			1/2 & u_2\\
			3/4 & u_3
		\end{cases}
	\end{gather*}\\
	\begin{gather*}
		\hat{p}_{\text{post}} = \frac{1}{4}u_1 + \frac{1}{2}u_2 + \frac{3}{4}u_3
	\end{gather*}	
	\end{multicols}
\end{example-N}
Sometimes, we will find priors and posteriors and likelihoods such that prior and posterior belong to some family $\mathcal{F}$ and the likelihood belongs to $\mathcal{G}$. Here, we say $\mathcal{F}$, $\mathcal{G}$ are conjugate families of priors\\\\
Case when we have continuous distributions and want to obtain posterior densities:
\begin{gather*}
	f(\theta) = \text{prior density}\\
	f(x_1, \ldots, x_n| \theta) = \text{ likelihood}\\
	f(\theta | x_1, \ldots, x_n) = \frac{f(x_1, \ldots, x_n| \theta) \cdot f(\theta)}{\underbrace{\boxed{\int f(x_1, \ldots, x_n| \theta) \cdot f(\theta) d\theta}}_{\text{function of observed data} \rightarrow C(X_1, \ldots, X_n)}}
\end{gather*}
The denominator is a function of observed data i.e. it is a \emph{normalizing constant in the posterior density}. Often we don't have to calculate it explicitly! \textbf{Note} that the posterior density depends on the data. It is however a density for $\theta$. So often, we will want to manipulate the posterior density into a recognizable form as a function of $\theta$ with moments that might depend on the data. 