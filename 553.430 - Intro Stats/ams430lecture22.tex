\classheader{2018-11-28}
\begin{itemize}
	\item Sums of Squares decomposition
	\item ANOVA tables
\end{itemize}
\subsection*{One-way ANOVA}
\begin{gather*}
	\textbf{Model:} \quad Y_{ij} \sim \mathcal{N}(\overbrace{\mu + \alpha_i}^{\mu_i}, \sigma^2)\\
	\sum_{i=1}^I \alpha_i = 0, \quad Y_{ij} \text{'s are independent}
\end{gather*}
Wish to test $H_0$: $\mu_i$ all equal $\Leftrightarrow$ $H_0: \alpha_i \equiv 0$ $\forall i$
\begin{align*}
	TSS = SSTot = SST & = \sum_i \sum_j (Y_{ij} - \bar{Y}_{\cdot \cdot})^2\\
	& = \sum_i \sum_j (Y_{ij} - \bar{Y}_{i \cdot} + \bar{Y}_{i \cdot} - \bar{Y}_{\cdot \cdot})^2\\
	& = \underbrace{\sum_i \sum_j (Y_{ij} - \bar{Y}_{i \cdot})^2}_{SS_W} + \underbrace{\sum_i \sum_j (\bar{Y}_{i\cdot} - \bar{Y}_{\cdot \cdot})^2}_{SS_B} + 2 \sum_i \sum_j \cancelto{0}{(Y_{ij} - \bar{Y}_{i \cdot})} (\bar{Y}_{i \cdot} - \bar{Y}_{\cdot \cdot})
\end{align*}
\begin{gather*}
	\bar{Y}_{i \cdot} = \frac{\sum_{j=1}^J Y_{ij}}{J} \qquad \text{Note: } J \bar{Y_{i\cdot}} = \sum_{j=1}^J Y_{ij}\\
	\text{Hence} \quad \sum_{i=1}^I Y_{i \cdot } J = \sum_i \sum_j Y_{ij}\\
	\frac{\sum_{i=1}^I Y_{i \cdot } \cancel{J}}{I \cancel{J}} = \frac{\sum_i \sum_j Y_{ij}}{IJ} = \bar{Y}_{\cdot \cdot}\\
	\text{That is, } \bar{Y}_{\cdot \cdot} = \frac{\sum_{i=1}^I \bar{Y}_{i \cdot} }{I}
\end{gather*}
Note that if $\alpha_i \equiv 0$, i.e. if $H_0$ is true, 
\begin{gather*}
	\bar{Y}_{i\cdot} \sim \mathcal{N}(\mu, \sigma^2/J) \text{ and $\bar{Y}_{i\cdot}$ r.vs are independent}
\end{gather*}
So let $W_i = \bar{Y}_{i\cdot}$ and 
\begin{gather*}
	\bar{W} = \frac{1}{I} \sum_{i=1}^I \bar{Y}_{i \cdot} = \frac{1}{I} \sum_{i=1}^I W_i = \bar{Y}_{\cdot \cdot}
\end{gather*}
We are i.i.d under $H_0$, $W_i \sim \mathcal{N}(\mu, \sigma^2/J)$ \quad $(I-1)S_W^2$
\begin{gather*}
	\text{So} \quad \frac{\sum_{i=1}^I (\bar{Y}_{i \cdot} - \bar{Y}_{\cdot \cdot})^2}{I-1} = \frac{1}{I-1} \sum_{i=1}^I (W_i - \bar{W})^2\\
	\text{So we conclude} \quad \boxed{\frac{J \sum(W_i - \bar{W})^2}{\sigma^2} \sim \chi^2(I-1)}\\
	\text{Note that } J \sum(W_i - \bar{W})^2 = \sum_i \sum_j (\bar{Y}_{i \cdot} - \bar{Y}_{\cdot \cdot})^2\\
	\text{Now, } \frac{SSB}{\sigma^2} \sim \chi^2(I-1) \quad \text{under $H_0$}
\end{gather*}
Next, what about
\begin{gather*}
	\sum_{i=1}^I \underbrace{\sum_{j=1}^J (Y_{ij} - \bar{Y}_{i \cdot})^2}_{\text{sample var $S_i^2(J-1)$}}\\
	\text{Note:} \quad \frac{(J-1)S_i^2}{\sigma^2} \sim \chi^2(J-1)
\end{gather*}
So Since $S_i^2$'s are independent, 
\begin{gather*}
	\sum_{i=1}^{I} \frac{(J-1)S_i^2}{\sigma^2} \sim \chi^2 (I(J-1))
\end{gather*}
Hence,
\begin{gather*}
	\frac{SS_W}{\sigma^2} \sim \chi^2(I(J-1))
\end{gather*}
\underline{Exercise} (HW). Show that $SS_W$ and $SS_B$ are independent. Assuming independence, under $H_0$,
\begin{gather*}
	\frac{SS_B / (I-1)}{SS_W / I(J-1)} \sim F(I-1, I(J-1))
\end{gather*}
$SS_B$ is sometimes called the $SSTr$, it encapsulates differences across sample treatment means $\bar{Y}_{i \cdot}$.\\\\
$SS_W$ is a proxy for inherent variability error in model (i.e. $\sigma^2$). And $SS_W$ is sometimes called "sums of squares for error" or $SSE$
\subsection*{ANOVA Table}
\begin{gather*}
	\frac{TSS}{\sigma^2} = \frac{\sum \sum (Y_{ij} - \bar{Y}_{\cdot \cdot})^2}{\sigma^2} \sim \chi^2 (IJ - 1) \quad \textbf{under $h_0$}
\end{gather*}

\subsection*{Benefits of ANOVA F-test}
\begin{enumerate}[label=\protect\circled{\alph*}]
	\item "Omnibus" test: can test equality of multiple means $\mu_1 = \mu_2 = \mu_3 \cdots = \mu_I$ vs atleast one pair differs at fixed level $\alpha$
	\item Less burdensome than pairwise t-test
	\item Type I error is controlled
\end{enumerate}
\underline{Drawback:} F-test doesn't allow us to immediately identify which means are different (if we reject $H_0$, for example)\\\\
When comparing two populations, the t-test for equality of means, is equivalent to the F-test, because $t^2 = F$
\subsection*{Tuky's Test}
Commonly used to identify which means might be different\\\\
\underline{Basic idea:}
\begin{enumerate}
	\item Order the sample treatment $\bar{Y_{i\cdot}}$
	\item Calculate a threshold value
	\item Check if $| \bar{Y}_{i_1 \cdot} - \bar{Y}_{i_2 \cdot} | > \omega$ or not
	\item If yes, reject $H_0 (i_1, i_2): \mu_{i_1} = \mu_{i_2}$
\end{enumerate}
The threshold value $\omega$ depends on the so called \underline{studentized range distribution}.\\\\
This is distribution of the random variable
\begin{gather*}
	Q = \underset{i_1, i_2}{\max} \frac{|(\bar{Y}_{i_1 \cdot} - \mu_{i_1}) - (\bar{Y}_{i_2 \cdot} - \mu_{i_2}) |}{S_p / \sqrt{J}}\\
	S_p^2 = \text{ pooled sample variance} = MSE = \frac{SS_W}{I(J-1)}
\end{gather*}
Note that $Q$ depends on two parameters:
\begin{enumerate}
	\item how many treatments? ($I$)
	\item how many $df$s in MSE ($I(J-1)$) Need equal treatment sizes.
\end{enumerate}
At level $\alpha$, you can locate (in table in Rice)
\begin{gather*}
	q_\alpha (I, I(J-1)) \quad I \text{ is treatments and } I(J-1) \text{ is df}
\end{gather*}
So,
\begin{gather*}
	\omega = q_\alpha (I, I(J-1)) \frac{S_p}{\sqrt{J}}
\end{gather*}
and we reject $H_0$: $\mu_{i_1} = \mu_{i_2}$ if $| \bar{Y}_{i_1 \cdot} - \bar{Y}_{i_2 \cdot} | > \omega$