\classheader{2018-09-24}
\begin{itemize}
	\item Properties of estimation
	\item Method of moments
	\item Maximum Likelihood
	\item Properties of estimators
\end{itemize}
\section*{Properties of Estimation}
Let $X_i$, $1 \leq i \leq n$, be i.i.d. random variables with some cdf $F_\theta$, where $\theta \subseteq \mathbb{R}^d$ is deterministic but potentially unknown vector.\\\\
We will often consider $X_i$'s with a pdf or pmf $f_\theta$ as well.
\begin{example}
	\begin{enumerate}
		\item $X_i$'s are i.i.d. Bernoulli ($p$), $p$ is unknown pmf: $P(X = 1) = p$, $P(X = 0) = 1-p$. How to estimate $p$ if we observe $X_1, \ldots, X_n$?
		\item $X_i$'s are i.i.d. Poission($\lambda$), $\lambda > 0$. How to estimate $\lambda$ given $X_1, \ldots, X_n$?
		\item $X_i$'s are i.i.d. Exp($\lambda$), $\lambda > 0$. How to estimate $\lambda$ given $X_1, \ldots, X_n$?
		\item $X_i$'s are i.i.d. Uniform$[0, \theta]$. How to estimate $\theta$ given $X_1, \ldots, X_n$? - What if $X_i$'s are i.i.d. Unif$[\alpha, \beta]$. How to estimate $\alpha$, $\beta$ then $X_1, \ldots, X_n$?
		\item $X_i$'s are i.i.d. Gamma$[\alpha, \beta]$. How to estimate $(\alpha, \beta)$ given $X_1, \ldots, X_n$? What if one of $\alpha, \beta$ is known?
		\item Let $X_1, \ldots, X_n \in \mathbb{R}^d$ be i.i.d. multivariate normal with mean vector $\vec{\mu} \in \mathbb{R}^d$ and covariance matrix $\Sigma \in \mathbb{R}^{dxd}$. How to estimate $\mu_d$ and $\Sigma$ from $X_1, \ldots, X_n$
	\end{enumerate}
\end{example}
In all cases, we are concerned with estimating the parameters associated to a cdf or density whose functional form we have specified and from which we have an i.i.d. sample. \\\\
If we did not specify the correctional form of $F_\theta$, e.g. did not specify "normal", then the inference at $F/t$ itself is classified as "non-parametric" inference.\\\\
According to laws of large numbers, both these lend credence to the idea that if we wish to estimate $\mu = E[X_i]$, $\bX$ is a reasonable start.
\begin{definition}
	The \underline{population moment} is defined as
	\begin{align*}
		\mu^{(k)} & = \mathbb{E}[X_i^k]\\
		& = \int x^k f(x) dx - \quad \text{given all data i.e. entire population.}
	\end{align*}
\end{definition}
Observe that $Y_i = X_i^k \sim $ i.i.d. and $\mathbb{E}[Y_i] = E[X_i^k]$\\\\
So laws of large numbers apply to $\bar{Y}$ and suggest:
\begin{gather*}
	\bar{Y} = \frac{\sum X_i^k}{n} = \frac{\sum Y_i}{n} = \text{kth sample moment of } X_i
\end{gather*}
is a reasonable estimate for $\mu^{(k)}$
\subsection*{Method of Moments estimators}
Suppose we are interested in $d$ parameters $\alpha_1, \ldots, \alpha_d$ (need not be population moments themselves)\\\\
\underline{Step 1} - This system related population moments to parameters $\alpha_1, \ldots, \alpha_d$
\begin{gather*}
	\mu^{(1)} = g_1(\alpha_1, \ldots, \alpha_d)\\
	\vdots\\
	\mu^{(d)} = g_d(\alpha_1, \ldots, \alpha_d)\\
\end{gather*}
\underline{Step 2} - Invert this to solve for $\alpha_1$ in terms of $\mu^{(1)}, \ldots, \mu^{(d)}$
\begin{gather*}
	\alpha_1 = h_1(\mu^{(1)}, \ldots, \mu^{(d)})\\
	\vdots\\
	\alpha_d = h_d(\mu^{(1)}, \ldots, \mu^{(d)})\\
\end{gather*}
\underline{Step 3} - Now if $h_1$ functions are regular enough (continuous, differentiable, etc.). Then again by laws of large numbers, we can find $\alpha_1, \ldots, \alpha_d$
\begin{example-N}
	Let $X_i$'s be i.i.d. $\mathcal{N}(\mu, \sigma^2)$ Calculate MOM estimators for $\mu, \sigma^2$
	\begin{gather*}
		\alpha_1 = \mu = \mu^{(1)} = \bX \qquad \alpha_2 = \sigma^2 = \mu^{(2)} - (\underbrace{\mu^{(1)} }_{\bX})^2\\
		\hat{\alpha_1}_{\text{MOM}} = \bX \qquad \hat{\alpha_2}_{\text{MOM}} = \mu^{(2)} - (\underbrace{\mu^{(1)} }_{\bX})^2
	\end{gather*}
\end{example-N}
\begin{example-N} Suppose $X_i$'s are uniform ($0, \theta$)
\begin{gather*}
	\mu^{(1)} = \frac{\theta - 0}{2} \Rightarrow \theta = 2 \mu^{(1)} \qquad \hat{\theta}_{\text{MOM}} =2 \bX
\end{gather*}
\end{example-N}
\begin{example-N} Suppose $X_i$'s are exp($\lambda$)
\begin{gather*}
	\mu^{(1)} = \frac{1}{\lambda} = \bX \qquad \hat{\lambda}_{\text{MOM}} = \frac{1}{\bX}
\end{gather*}
\end{example-N}
\begin{example-N}
	Let $X \sim $ Gamma($\alpha, \lambda$)
	\begin{gather*}
		E[X] = \mu^{(1)}= \frac{\alpha}{\lambda} \qquad E[X^2] = 
		\mu^{(2)} = \frac{\alpha(\alpha + 1)}{\lambda^2} = \mu^{(1)^2} + \frac{\mu^{(1)}}{\lambda}\\
		\hat{\alpha}_{\text{MLE}} = \frac{\hat{\mu}^{(1)}}{\hat{\mu}^{(2)} - \hat{\mu}^{(1)^2}} \qquad \hat{\lambda}_{\text{MLE}} = \frac{\hat{\mu^{(1)}}}{\hat{\mu^{(2)}} \hat{\mu^{(1)^2}}}
	\end{gather*}
\end{example-N}