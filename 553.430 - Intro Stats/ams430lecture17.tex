\classheader{2018-10-31}
\begin{itemize}
	\item Neyman-Pearson
	\item Uniformly most powerfiul tests
	\item GLRTs
\end{itemize}
\subsection*{Neyman-Pearson Lemma}
\begin{theorem} (The Neyman-Pearson)\\
Let $H_0$, $H_1$ be simple, let $\circled{H}_0 \bigcup \circled{H}_a = \circled{H}$. Suppose LRT rejects $H_0$ when $LR \leq C$ and that this test procedure hhas significance level $\alpha$. Consider \underline{any other} test with significance less than or equal to $\alpha$. The power of this test is less than or equal to power of LRT.
\end{theorem}
\begin{proof}
	Since $H_0$, $H_a$ (or $H_1$) are both simple, let $f_0(x)$, $F_1(x)$ denote the respective densities under null and alternative. 	Any decision rule is of the form 
	\begin{gather*}
		d(x) = \begin{cases}
		0 & \text{if $H_0$ accepted}\\
		1 & \text{if $H_0$ rejected}
		\end{cases}\\
		\text{Note that} \qquad \mathbb{E}[d(\uX)] = P(d(\uX) = 1)\\
		\text{Note that significance level: } \qquad P(d(\uX) = 1 | H_0) = \mathbb{E}[d(\uX)]\\\\
		\textbf{Power:} \quad 1 - \beta = 1 - P(\text{Type II Error}) = P(d(\uX) = 1 | H_1) = E_1 (d(\uX))
	\end{gather*}
	Now, let's consider the particular decision rule given by LRT
	\begin{gather*}
		\textbf{Reject $H_0$ if }\quad \frac{f_0(\uX)}{f_1(\uX)} < c \qquad c \text{ is chosen so that } P(\text{Type II Error}) = \alpha\\
		E_0[d(\uX)] = \alpha \qquad \text{where $d(\uX)$ is the LRT decision rule}.
	\end{gather*}
	Let $d^*$ be any other decision rule with at most $\alpha$ as Type I error: $\mathbb{E}_0 [d^*(X)] \leq \alpha$\\\\
	It suffices to show:
	\begin{equation*}
		\underbrace{\mathbb{E}_1 [d^*(\uX)]}_{\text{power of $d^*$}} = \underbrace{\mathbb{E}_1 [d^(\uX)]}_{\text{power of LRT}}
	\end{equation*} 
\end{proof}
\subsubsection*{Key Inequality}
\begin{equation*}
	d^*(\ux) [cf_1(\ux) - f_0(\ux)] \leq \underbrace{d(\ux)}_{LRT}[cf_1(\ux) - f_0(\ux)]
\end{equation*}
We reject LRT, i.e. $d(\ux) = 1$, when 
\begin{gather*}
	f_0(\ux) < xf_1(\ux)\\
	cf_1(\ux) - f_0(\ux) > 0
\end{gather*}
So if $\ux$ is such that $d(\ux) = 1$, then 	
\begin{gather*}
	 cf_1(\ux) - f_0(\ux) > 0\\
	 \text{and} \qquad d^*(\ux) [cf_1(\ux) - f_0(\ux)] \leq cf_1(\ux) - f_0(\ux)
\end{gather*}
If $\ux$ is such that $d(\ux) = 0$, \quad then $d(\ux) [cf_1(\ux) - f_0(\ux)] = 0$.\\ But also since $d(\ux) = 0$, \quad $cf_1(\ux) - f_0(\ux) \leq 0$\\\\
Thus we now consider two options - either $d^*(\ux) = 0$, in which case:
\begin{equation*}
	d^*(\ux)[cf_1(\ux) - f_0(\ux)] = 0 \qquad \text{which leads to $0 = 0$}
\end{equation*}
If $d(\ux) = 0$ \&  $d^*(\ux) = 1$, we have
\begin{gather*}
	\underbrace{d^*(\ux)}_{1} \underbrace{[cf_1(\ux) - f_0(\ux)]}_{\text{non-positive}} \leq 0 = \overbrace{d(\ux)}^{0} [cf_1(\ux) - f_0(\ux)]\\
	\text{so we find} \qquad cd^*(\ux) f_1(\ux) - d^*(\ux) f_0(\ux) \leqslant cd(\ux) f_1(\ux) - d(\ux)f_0(x)
\end{gather*}
Let's note that, integrating over possible values $x_1, \ldots, x_n$ in the vector $\ux = (x_1, \ldots, x_n)$
\begin{gather*}
	c\mathbb{E}_1 (d^*(X)) - \mathbb{E}_0 (d^*(\uX)) \leq c \mathbb{E}_1(d(\uX)) - \mathbb{E}_0(d(\uX))\\
	\text{so note that} \quad \underbrace{\mathbb{E}_0 (d^*(\uX)) - \mathbb{E}_0(d(\uX))}_{\text{-ve if $d^*$ has small TI error than d}} \geqslant c\bigg(\mathbb{E}_1 (d^*(X)) - \mathbb{E}_1(d(\uX)) \bigg)
\end{gather*}
In which case
\begin{gather*}
	\mathbb{E}_1 (d^*(X)) - \mathbb{E}_1(d(\uX)) < 0\\
	\mathbb{E}_1 (d^*(X)) < \mathbb{E}_1(d(\uX))
\end{gather*}
\subsubsection*{Most powerful test}
\begin{example-N}
	$X_i$ i.i.d. $\mathcal{N}(\mu, \sigma^2)$, suppose $\sigma^2$ is known
	\begin{center}
		Consider $H_0$: $\mu = \mu_0$\\
		\qquad $H_a$: $\mu = \mu_a$\\
		Folk wisdom: use $\bX$ as T.S.
	\end{center}
	\begin{gather*}
		LRT = \frac{f_0(\uX)}{f_1(\uX)} = \frac{\cancel{\bigg( \frac{1}{\sigma\sqrt(2\pi)} \bigg)^n} \exp \bigg\{ \frac{-\sum_i (x_i - \mu_0)^2}{2\sigma^2} \bigg\} }{\cancel{\bigg( \frac{1}{\sigma\sqrt(2\pi)} \bigg)^n} \exp \bigg\{ \frac{-\sum_i (x_i - \mu_a)^2}{2\sigma^2} \bigg\} }\\
		\text{take logs}: \qquad \frac{-\sum_i (x_i - \mu_0)^2}{2\sigma^2} + \frac{-\sum_i (x_i - \mu_a)^2}{2\sigma^2}  \leq d\\
		\text{Reject if} \qquad 2\bX n \mu_0 - n \mu_0^2 - 2\bX n \mu_a + \mu_a^2 \leq d'\\
		= 2n\bX (\mu_0 - \mu_a) + n (\mu_a^2 - \mu_0^2) \leq d'
	\end{gather*}
	Reject $H_0$ if 
	\begin{gather*}
		\bX (\mu_0 - \mu_a) \leq \frac{d' - n(\mu_a^2 - \mu_0^2)}{2n}
	\end{gather*}
	Since $\mu_a > \mu_0$, we find reject $H_0$ if 
	\begin{gather*}
		\bX \geq \boxed{\frac{d' - n(\mu_a^2 - \mu_0^2)}{2n(\mu_0 - \mu_a)}} \tag{$\star$}
	\end{gather*}
	i.e. we reject $H_0$ if $\bX$ is sufficiently large. $\star$ looks complicated like it depends on $\mu_0, \mu_a$, etc.
	\begin{gather*}
		P(\text{Reject } H_0 | H_0) = \alpha\\
		P(\bX > \star | H_0) = \alpha\\
		\text{we know from previous lectures that } \bX \sim \mathcal{N}(\mu_0, \sigma^2 / n)\\
		= P \bigg(\frac{\bX - \mu_0}{\sigma / \sqrt{n}} > \frac{\star - \mu_0}{\sigma / \sqrt{n}} \bigg| H_0 \bigg) = \alpha \\
		= P \bigg(Z > \frac{\star - \mu_0}{\sigma / \sqrt{n}} \bigg) = \alpha
	\end{gather*}
	So
	\begin{gather*}
		\tcbhighmath[drop fuzzy shadow]{\star = Z_\alpha \frac{\sigma}{\sqrt{n}} + \mu_0}
	\end{gather*}
\end{example-N}
So we reject if \quad $\bX > Z_\alpha \frac{\sigma}{\sqrt{n}} + \mu_0$ \quad and we note that this rejection region is not dependent on explicit value of $\mu_a$, as long as $\mu_a > \mu_0$\\\\
So note that the exact same test (reject $H_0$ if $\bX > Z_\alpha \frac{\sigma}{\sqrt{n}} + \mu_0$) is \underline{most powerful} for $H_0: \mu = \mu_0$ vs $H_a: \mu = \mu_a$ for \underline{any choice} of $\mu_a > \mu_0$.\\\\
So this is a \underline{uniformly} most \underline{powerful} test (UMP) for 
\begin{center}
	$H_0: \mu = \mu_0$ (simple $H_a$)\\
	vs $H_0: \mu > \mu_0$ (comp. $H_a$)
\end{center}