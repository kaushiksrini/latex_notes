\classheader{2018-10-01}
\begin{itemize}
	\item MLEs - normal, gamma, uniform
	\item Modes of convergence
	\item Slutsky's Theorem
	\item Asymptotic properties of MLEs
\end{itemize}
\subsection*{MLEs - Normal Distribution}
Let $X_i, 1 \leq i \leq n$ be i.i.d. $\mathcal{N}(\mu, \sigma^2)$ The \underline{likelihood}
\begin{align*}
	f(x_1, \leq, x_n | \mu, \sigma^2) & = \prod_{i=1}^n f(x_i | \mu, \sigma^2)\\
	& = \prod_{i=1}^n \bigg[ \frac{1}{\sigma \sqrt{2\pi}} \exp \bigg( \frac{-(x_i - \mu)^2}{2\sigma^2} \bigg) \bigg]\\
	& = \bigg( \frac{1}{\sigma \sqrt{2\pi}} \bigg)^n \exp \bigg(\frac{-1}{2\sigma^2}  \sum_{i=1}^n (x_i - \mu)^2\bigg)\\
	\underline{\text{The log-likelihood:}} \qquad & = -n \log (\sigma \sqrt{2\pi}) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
\end{align*}
Maximize this w.r.t. $\mu, \sigma$. Here log-likelihood depends smoothly on parameters $\rightarrow$ can consider critical points as 1st step in maximization.
\begin{gather*}
	\frac{\partial l}{\partial \mu} = \frac{2}{2\sigma^2} \sum_{i=1}^n (x_i - \mu) = 0 \Longrightarrow n\mu = \sum_{i=1}^n x_i \Longrightarrow \boxed{\hat{\mu}_{\text{MLE}} = \bX}\\
	\frac{\partial l}{\partial \sigma} = \frac{-n \cancel{\sqrt{2\pi}}}{\sigma \cancel{\sqrt{2\pi}}} - \frac{-1}{\sigma^3} \sum_{i=1}^n (x_i - \mu)^2 = 0 \Longrightarrow \frac{n}{\sigma} = \frac{1}{\sigma^3} \sum_{i=1}^n (x_i - \mu)^2 \Longrightarrow \boxed{\hat{\sigma}_{\text{MLE}}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2}
\end{gather*}
Need to make sure two partial derivatives vanish simultaneously
\begin{gather*}
	\mu = \bX\\
	\sigma^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 \Longrightarrow \boxed{\hat{\sigma}_{\text{MLE}} = \sqrt{\frac{1}{n} \sum_{i=1}^n (X_i - \bX)^2}}
\end{gather*}
Capital $X_i$'s because want a function of the random variables in our sample. $E[\bX] = \mu$. So $\hat{\mu}$ MLE is \underline{unbiased}. Var($\hat{\mu}_{\text{MLE}}$) = $\frac{\sigma^2}{n}$\\\\
\textbf{Question:} Is $E[\hat{\sigma}_{\text{MLE}}] = \sigma$?
\begin{equation*}
	\text{Next, } \quad \hat{\mu}_{\text{MLE}} = \bX = \frac{1}{n} \sum_{i=1}^n X_i \sim \mathcal{N}\bigg(\mu, \frac{\sigma^2}{n} \bigg)
\end{equation*}
\subsection*{Support}
Given a density function $f(x|\theta)$, we define the \underline{support} of $f$ to be
\begin{equation*}
	\supp f = \{x : f(x|\theta) > 0 \}
\end{equation*}
Suppose $\Theta$ is the space (in $\mathbb{R}, \mathbb{R}^d$) to which $\theta$ belongs:
\begin{center}
	If $X_i$'s are i.i.d.. Bernoulli($p$), then $\Theta = (0,1),  \quad \supp f = \{0,1 \}$\\
	If $X_i$'s are i.i.d.. $\mathcal{N}(\mu, \sigma^2)$, then $\Theta = \{(a,b) : a \in \mathbb{R}, b > 0  \}, \quad \supp f = \mathbb{R}$\\
\end{center}
We say that the \underline{$\supp f$ is independent} of $\theta$ if
\begin{equation*}
	\{x : f(x | \theta) > 0 \} \quad \text{is the same set for all $\theta \in \Theta$}
\end{equation*}
\subsection*{MLEs - Uniform Distribution}
Now let $X_i$ be i.i.d. Unif$[0,\theta]$ \qquad $\theta > 0$ \qquad Here $\supp f$ is not independent of $\theta$.
\begin{equation*}
	\supp f = \{x: f(x | \theta) > 0 \} \qquad f(x|\theta) \begin{cases}
		0 & x < 0\\
		\frac{1}{\theta} & 0 \leq x \leq \theta\\
		0 & x >> \theta
	\end{cases}
\end{equation*}
\underline{Joint Likelihood}
\begin{gather*}
f(x_1, x_2, \ldots, x_n | \theta) = \underbrace{\frac{1}{\theta} \cdot \frac{1}{\theta} \cdots \frac{1}{\theta}}_{\text{n times}} = \bigg( \frac{1}{\theta} \bigg)^n\\
\text{with indicator} \qquad f(x_1, x_2, \ldots, x_n | \theta) = \bigg( \frac{1}{\theta} \bigg)^n \bigg( I_{[0, \theta]}(x_1) \cdot I_{[0, \theta]}(x_2) \cdots I_{[0, \theta]}(x_n) \bigg)\\
= \bigg( \frac{1}{\theta} \bigg)^n I_{\min (x_i) \geq 0, \text{ } \max (x_i) \leq \theta}
\end{gather*}
\textbf{Note: } $\big( \frac{1}{\theta}\big)^n $ is decreasing in $\theta$
\begin{itemize}
	\item So want to choose $\theta$ as small as possible
	\item So note lower bound on $\theta$ in terms of $x_i$'s if likelihood is to remain positive.
\end{itemize}
\begin{equation*}
	\boxed{\hat{\theta}_{\text{MLE}} = \underset{i \in \{1, \ldots, n\}}{\max} (x_i)}
\end{equation*}
\subsection*{Modes of Convergence}
Let X be $unif[0,1]$. Let $g_n(x) = nI_{[0, 1/n]}(x) $\\
Let $Y_n = g_n(X)$
\begin{center}
	If $X = 0, g_n(0) = n$ \quad (grows unboundedly)\\
	If $X = x \in (0, 1]$, $g_n(x)$ is eventually 0.
\end{center}
If $X > 0, g_n(X) \rightarrow 0$. $X = a > 0$. if $n$ large enough so $\frac{1}{n} < a$, then $g_n(a) = 0$\\
For all $\omega$ except $\omega = 0$, $Y_n(\omega) \rightarrow 0:$ $P(\{ \omega = 0 \}) = 0$\\
So we have set A. A = $\{\omega : \omega > 0 \}$ with $P(A) = 1$, such that $\forall \omega \in A$, $Y_n(\omega) \rightarrow 0$. So $Y_n \rightarrow 0$ with probability 1. 