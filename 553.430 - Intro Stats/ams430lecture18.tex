\classheader{2018-11-05}
\begin{itemize}
	\item Hypothesis tests/Confidence Intervals.
	\item Bayesian HTs.
	\item GLRTs and Wilks Theorem.
	\item Distributions based on normal.
	\item Midterm II next Wednesday
\end{itemize}
\subsection*{Confidence Intervals}
Last time we considered $X_i \sim$ i.i.d. $\mathcal{N}(\mu, \sigma^2)$, $\sigma^2$ known
\begin{center}
	$H_0: \mu = \mu_0$ \qquad $H_a: \mu > \mu_0$
\end{center}
We found that if we considered
\begin{gather*}
	\frac{\bX - \mu_0}{\sigma / \sqrt{n}} \qquad \text{as our test statistic with rejection region}\\
	T > Z_{\alpha}, \qquad \underbrace{\text{i.e. reject $H_0$ if } \bX > Z_{\alpha} \frac{\sigma}{\sqrt{n}} + \mu_0}_{\text{test procedure is uniformly most powerful}}
\end{gather*}
Now, what if we had instead considered a two sided test?
\begin{center}
	$H_0: \mu = \mu_0$ \qquad $H_a: \mu \neq \mu_0$
\end{center}
Here might consider rejecting $H_0$ if $| \bX - \mu_0|$ is sufficiently large. i.e.
\begin{gather*}
	\frac{\bX - \mu_0}{\sigma / \sqrt{n}} \sim \mathcal{N}(0, 1) \text{ under } H_0\\
\text{So }	\textbf{reject} \text{ if} \quad \frac{\bX - \mu_0}{\sigma / \sqrt{n}} < -Z_{\alpha / 2} \quad \text{or}  \quad \frac{\bX - \mu_0}{\sigma / \sqrt{n}} > Z_{\alpha / 2}
\end{gather*}
So we reject if
\begin{gather*}
	\bX < -Z_{\alpha/2} \frac{\sigma}{\sqrt{n}} + \mu_0\\
	\bX > Z_{\alpha/2} \frac{\sigma}{\sqrt{n}} + \mu_0\\
\end{gather*}
So we accept $H_0$ if 
\begin{gather*}
		\tcbhighmath[drop fuzzy shadow]{-Z_{\alpha/2} \frac{\sigma}{\sqrt{n}} + \mu_0 < \bX < Z_{\alpha/2} \frac{\sigma}{\sqrt{n}} + \mu_0}
\end{gather*}
Suppose we want a random interval that contains the population parameter $\mu$ (whatever its value) with probability $1 - \alpha$ i.e. suppose we have some population parameter (in this case $\mu$) whose value we'd like to estimate.\\\\
A $(100)(1-\alpha)$ \% C.I. for $\mu$ is a random interval containing $\mu$ with specified probability $1-\alpha$\\\\
Note that a CI for $\mu$ \circled{a} level $\alpha$ looks like
\begin{gather*}
	\bigg(\bX - Z_{\alpha / 2} \frac{\sigma}{\sqrt{n}}, \bX + Z_{\alpha / 2} \frac{\sigma}{\sqrt{n}} \bigg)
\end{gather*}
and we can say we accept $H_0: \mu = \mu_0$ when $100(1-\alpha)$\% CI given by $\bigg(\bX - Z_{\alpha / 2} \frac{\sigma}{\sqrt{n}}, \bX + Z_{\alpha / 2} \frac{\sigma}{\sqrt{n}} \bigg)$ contains $\mu_0$\\\\
That is, we have a duality between CIs and HTs.
\begin{theorem}
	Suppose for every $\theta_0 \in \Theta$, $\exists$ a level $\alpha$ test of $H_0: \theta = \theta_0$. Suppose $A(\theta_0) = \{ \uX : \text{ decision rule is to accept $H_0$} \}$. Then let $C(X) = \{ \theta \in \Theta : X \in A(\theta) \}$. Then $C(X)$ is a $100(1 - \alpha)$\% confidence region for $\theta$.
\end{theorem}
Conversely,
\begin{theorem}
	Suppose that $C(X)$ is a $100(1 - \alpha)$\% confidence region for $\theta$. i.e.
	\begin{gather*}
		P ( \theta_0 \in C(X) | \theta = \theta_0 ) = 1 - \alpha
	\end{gather*} for every $\theta_0$. Then if we define 
	\begin{gather*}
		A(\theta_0) = \{ \uX : \theta_0 \in C(X) \}
	\end{gather*}
	this is an acceptance region for a level $\alpha$ test of $H_0: \theta = \theta_0$
\end{theorem}
\subsection*{Bayesian Hypotheses Tests}
Consider $X_i$'s i.i.d. $f(x | \theta)$, suppose we now have a probability distribution over hypotheses: let $H_0$ and $H_1$ be two simple null and alternative hypotheses (respectively) and let $\pi_0 = P(H_0)$ and $\pi_1 = P(H_1)$. So in the Bayesian framework, we observe a vector of data and then \underline{update} $\pi_0$ and $\pi_1$
\begin{gather*}
	\text{Compute} \quad P(H_1 | X_1, \ldots, X_n) = \frac{P(X_1, \ldots, X_n | H_1) \pi_1}{P(X_1, \ldots, X_n | H_0) \pi_0 + P(X_1, \ldots, X_n | H_1) \pi_1}\\
	P(H_0 | X_1, \ldots, X_n) = \frac{P(X_1, \ldots, X_n | H_0) \pi_0}{P(X_1, \ldots, X_n | H_0) \pi_0 + P(X_1, \ldots, X_n | H_1) \pi_1}\\
\text{Decision Rule: } P(H_0 | X_1, \ldots, X_n) > P(H_1 | X_1, \ldots, X_n)
\end{gather*}
Observe that
\begin{gather*}
	\frac{P(H_0 | X_1, \ldots, X_n)}{P(H_1| X_1, \ldots, X_n)} = \frac{P(X_1, \ldots, X_n | H_0) \pi_0}{P(X_1, \ldots, X_n | H_1) \pi_1}
\end{gather*}
Accept $H_0$ if $\star$ is greater than some constant.\\\\
So we are still comparing likelihoods, i.e. computing a L.R. 
\subsubsection*{Detour now into Rice, Ch 6}
Distributions derived from the normal distribution.\\\\
Suppose $\uX \in \mathbb{R}^d$ has a multivariate normal distribution, so $\uX$ has density
\begin{gather*}
	f_{\uX}(\underline{t}) = C \exp \bigg( -\frac{1}{2} (\underline{t} - \underline{\mu})^T \Sigma^{-1} (\underline{t} - \underline{\mu}) \bigg) \quad \text{where} \quad \underline{t} = \begin{bmatrix}
		t_1\\ \vdots\\ t_d
	\end{bmatrix} \quad  \underline{\mu} = \begin{bmatrix}
		\mathbb{E}[X_1]\\ \vdots \\ \mathbb{E}[X_d]
	\end{bmatrix} \qquad \Sigma_{ij} = cov(X_i, X_j) 
\end{gather*}
\textbf{Facts (lemmas):} 
\begin{enumerate}[label=\protect\circled{\arabic*}]
	\item If $\uX \sim$ jointly normal and $\sigma_{ij} = 0$, then $X_i, X_j$ are independent. t
	\item If $\uX$ is normal and $O \in \theta(dxd)$, so $O^T O = I$ the $OX$ is normal (invariance of normality under rotation). 
	\item If $X_1, \ldots, X_d$ are separately normal and independent, then $\uX = \begin{bmatrix}
		x_1\\ \vdots\\ x_d
	\end{bmatrix}$ is jointly normal and $\Sigma$ is diagonal.
	\item if $\uX = (X_1, \ldots, X_n)$ where $X_i$'s are i.i.d. normal then $\bX$ and $s^2$ are independent.
\end{enumerate}
\subsubsection*{Chi-squared}
If $Z_1, \ldots, Z_n$ are i.i.d. $\mathcal{N}(0,1)$, then
\begin{gather*}
	\sum_{i=1}^n Z_i^2 \sim \chi^2 \quad \text{n degrees of freedom}	
\end{gather*}
Degrees of Freedom in a $\chi^2$ correspond to number of independent squared normals in the sum.\\\\
Finally, if $X_1, \ldots, X_n$ is i.i.d. $\mathcal{N}(0, 1)$, then
\begin{gather*}
	(n-1)s^2 = (n-1) \bigg[ \frac{1}{n-1} \sum_{i=1}^n (X_i - \bX)^2 \bigg] = \sum_{i=1}^n (X_i - \bX)^2 \sim \chi^2_{n-1}
\end{gather*}