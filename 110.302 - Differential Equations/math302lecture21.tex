\classheader{2018-08-21}
We have already seen that $\vec{x} = \begin{bmatrix} 1\\ 2\end{bmatrix}$ is an eigenvector of the eigenvalue 3 for $A = \begin{bmatrix} 1 & 1\\ 6 & 0 \end{bmatrix}$\\
What is an eigenvector for $\lambda = -2$?\\
Back to $A\vec{x} = \lambda \vec{x}$:
\begin{gather*}
	\begin{bmatrix}
		1 & 1\\ 6 & 0
	\end{bmatrix}
	\begin{bmatrix}
		x_1\\x_2
	\end{bmatrix} = -2
	\begin{bmatrix}
		x_1\\x_2
	\end{bmatrix}\\
	\begin{split}
		\begin{rcases}
			x_1 + x_2 = -2 x_1\\
			6x_1 \quad \quad = -2x_2
		\end{rcases}
	\end{split} \Rightarrow
	\begin{split}
		3x_1 = -x_2\\
		6x_1 = -2x_2
	\end{split} 
\end{gather*}
Notice how this system is degenerate (has lots of solutions!) \underline{It always is}...\\
\subsubsection*{Back to ODEs:}
If $\vec{x} = \begin{bmatrix}
	x_1\\ \vdots \\ x_n
\end{bmatrix}$ is a vector of variables (functions of time), then $\vec{x}' = \begin{bmatrix}
	x_1'(t)\\ \vdots \\ x_n'(t)
\end{bmatrix}$ is its derivative, and 
\begin{gather*}
	x_1' = p_{11}(t)x_1 + \cdots + p_{1n}(t)x_n + g_1(t)\\
			x_2' = p_{21}(t)x_1 + \cdots + p_{2n}(t)x_n + g_2(t)\\
			\quad \quad \quad \vdots\\
			x_n' = p_{n1}(t)x_1 + \cdots + p_{nn}(t)x_n + g_n(t)\\
\end{gather*}
is a linear system with matrix form...
\begin{equation*}
	\vec{x}' = \mathbb{P}(t)\vec{x} + \vec{g(t)}
\end{equation*}
A solution is a vector of functions $\vec{x}(t) = \begin{bmatrix}
	x_1(t)\\ \vdots \\ x_n(t)
\end{bmatrix}$ and a set of solutions (if more than one is)
\begin{equation*}
	\vec{x}^{(1)} (t) = 
	\begin{bmatrix}
		x_1^{(1)}(t)\\
		\vdots\\
		x_n^{(1)}(t)
	\end{bmatrix}, \cdots, 
	\vec{x}^{(b)} (t) = 
	\begin{bmatrix}
		x_1^{(b)}(t)\\
		\vdots\\
		x_n^{(b)}(t)
	\end{bmatrix}. 
\end{equation*}
Note the notational confusion, We \underline{will not deal with} nth order systems, so the context is nice.\\\\
\underline{Some facts} - Let $\vec{x}' = \mathbb{P}(t)\vec{x}$ be a hhomogenous linear system ($\vec{g(t)} = \vec{0}$)
\begin{enumerate}[label=\protect\circled{\arabic*}]
	\item Superposition holds if $\vec{x}^{(1)}(t)$ and $\vec{x}^{(2)}(t)$ both solve $\vec{x}' = \mathbb{P}(t)\vec{x}$, then so does
	\begin{equation*}
		c_1 \vec{x}^{(1)} + c_2 \vec{x}^{(2)}
	\end{equation*}
	for any choice of $c_1, c_2 \in \mathbb{R}$ (Any linear combination of solutions is a solution!)
	\begin{example-N}
		$\vec{x}' = \begin{bmatrix}
			1 & 1\\ 6 & 0
		\end{bmatrix} \vec{x}$. Here $\mathbb{P}(t) = \begin{bmatrix}
			1 & 1\\ 6 & 0
		\end{bmatrix}$ are \underline{constants}. Hence verify that:
		\begin{enumerate}[label=\protect\circled{\arabic*}]
			\item $\vec{x}^{(1)} = \begin{bmatrix}
				1\\ 2
			\end{bmatrix} e^{3t} = \begin{bmatrix}
				e^{3t}\\ 2e^{3t}
			\end{bmatrix}$ is a solution
			\item $\vec{x}^{(2)} = \begin{bmatrix}
				1\\ -3
			\end{bmatrix} e^{-2t} = \begin{bmatrix}
				e^{-2t}\\ -3e^{-2t}
			\end{bmatrix}$ is also a solution
		\end{enumerate}
		Hence so is $c_1 \begin{bmatrix}
				1\\ 2
			\end{bmatrix} e^{3t} + c_2 \begin{bmatrix}
				1\\ -3
			\end{bmatrix} e^{-2t}$.
		\begin{equation*}
			\text{For \circled{2}, } \quad \vec{x}' = \dfrac{d}{dt}
			\begin{bmatrix}
				e^{-2t}\\
				-3e^{-2t}
			\end{bmatrix} = 
			\begin{bmatrix}
				e^{-2t} - 3e^{-2t}\\
				6e^{-2t} + 0(-)e^{-2t}
			\end{bmatrix} = 
			\begin{bmatrix}
				x_1 + x_2 \\ 6x_1
			\end{bmatrix}
		\end{equation*}
	\end{example-N}
	\item In $\mathbb{R}^n$, there can be at most $n$ linearly independent vectors: For example:
	\begin{equation*}
		\vec{e_1} = \begin{bmatrix}
			1\\ 0\\ \vdots \\ 0
		\end{bmatrix}, \quad 
		\vec{e_2} = \begin{bmatrix}
			0\\ 1\\ \vdots \\ 0
		\end{bmatrix}, \quad \cdots,
		\vec{e_n} = \begin{bmatrix}
			0\\ 0\\ \vdots \\ 1
		\end{bmatrix}, \quad 
	\end{equation*}
	It is reasonable to conclude that there can be up to n-linearly independent solutions to $\vec{x}' = \mathbb{P}(t)\vec{x}$
	\begin{equation*}
		\vec{x}^{(1)}(t) = 
		\begin{bmatrix}
			x_1^{(1)}(t)\\ \vdots \\ x_n^{(1)}(t)
		\end{bmatrix}, \cdots, \quad \vec{x}^{(n)}(t) =  
		\begin{bmatrix}
			x_1^{(n)}(t)\\ \vdots \\ x_n^{(n)}(t)
		\end{bmatrix}
	\end{equation*}
	They will be independent on some interval I if for $t \in I$,
	\begin{equation*}
		c_1 \vec{x}^{(1)}(t) + \cdots + c_n\vec{x}^{(n)}(t) = \vec{0}
	\end{equation*}
	Can only be solved by $c_1 = c_2 = \cdots = c_n = 0$\\
	Combine all of these vector solutions as columns in a single $nxn$ matrix
	\begin{equation*}
		\Xlines(t) = 
		\begin{bmatrix}
			\vec{x}^{(1)}, \cdots, \vec{x}^{(n)}
		\end{bmatrix} = 
		\begin{bmatrix}
			x_1^{(1)} & \cdots & x_1^{(n)}\\
			\vdots & \ddots & \vdots\\
			x_n^{(1)} & \cdots & x_n^{(n)}
		\end{bmatrix}
	\end{equation*}
	Then $\det \Xlines \neq 0$ iff \underline{all columns are independent}.
\end{enumerate}
\begin{definition}
	$\det \Xlines(t)$ is called the \underline{Wronskian (determinant)} of the solution set, and denoted $W(\vec{X}^{(1)}, \cdots, \vec{X}^{(n)})$
\end{definition}
\begin{theorem}
	If $\vec{X}^{(1)}, \cdots, \vec{X}^{(n)}$ are all solutions to $\vec{x}' = \mathbb{P}(t)\vec{x}$ on $I = (\alpha, \beta) \in \mathbb{R}$, then for all $t \in I$, either $W(\vec{X}^{(1)}, \cdots, \vec{X}^{(n)})$ is identically 0 or is never 0.
\end{theorem}
\begin{definition}
	If $\vec{X}^{(1)}, \cdots, \vec{X}^{(n)}$ one all solutions to the n-dimension $\vec{x}' = \mathbb{P}(t)\vec{x}$ on I and $W(\vec{X}^{(1)}, \cdots, \vec{X}^{(n)}) \neq 0$ on I then \circled{a} $\Xlines(t)$ is called \underline{fundamental set of solutions}. and \circled{b} $\vec{Y(t)} = c_1\vec{x}^{(1)} + \cdots + c_n\vec{x}^{(n)}$ is the \underline{general solution}.
\end{definition}
\begin{example-N}
	Given $\vec{x}' = \begin{bmatrix}
			1 & 1\\ 6 & 0
		\end{bmatrix} \vec{x}$, with solutions
	\begin{gather*}
		\begin{split}
		\vec{x}^{(1)} = \begin{bmatrix}
			1 \\2
		\end{bmatrix} e^{3t} = 
		\begin{bmatrix}
			e^{3t} \\ 2e^{3t}
		\end{bmatrix},
		\end{split} \quad 
		\begin{split}
			\vec{x}^{(2)} = \begin{bmatrix}
			1 \\-3
		\end{bmatrix} e^{-2t} = 
		\begin{bmatrix}
			e^{-2t} \\ -3e^{-2t}
		\end{bmatrix}
		\end{split}\\
		\text{Since } W(\vec{x}^{(1)}, \vec{x}^{(2)} ) = 
		\begin{vmatrix}
			e^{3t} & e^{-2t}\\
			2e^{3t} & -3e^{-2t}
		\end{vmatrix} = -3e^{3t}e^{-2t} - 2e^{3t}e^{-2t} = -5e^{t} \neq 0 \text{ on } \mathbb{R}
	\end{gather*}
	\begin{gather*}
		\begin{split}
		\underbrace{\Xlines(t) = 
		\begin{vmatrix}
			e^{3t} & e^{-2t}\\
			2e^{3t} & -3e^{-2t}\\
		\end{vmatrix}}_
		{\text{\underline{Fundamental set of solutions}}}
		\end{split} \quad \text{\underline{and}} \quad 
		\begin{split}
			\vec{\varphi(t)} = c_1
			\begin{bmatrix}
				e^{3t}\\ 2e^{3t}
			\end{bmatrix} + c_2
			\begin{bmatrix}
				e^{-2t}\\ -3e^{-2t}
			\end{bmatrix}\\
			\underbrace{= c_1
			\begin{bmatrix}
				1\\2
			\end{bmatrix} e^{3t} + c_2
			\begin{bmatrix}
				1\\-3
			\end{bmatrix} e^{-2t}}_{\text{as the \underline{general solution}}}
		\end{split} 
	\end{gather*}
\end{example-N}